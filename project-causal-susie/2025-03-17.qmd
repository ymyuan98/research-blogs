---
title: "17 March 2025"
format: 
  gfm:
    html-math-method: webtex
editor: visual
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## 1. Inverse probability weighting estimates for categorical treatment with 3 levels

### 1.1 Definition of the potential outcome model

Recall Section 1.2 of `2025-03-12.qmd`.

Suppose the exposure has three levels, $x=0, 1, 2$. Assume the confounder is a random variable $U$. The data generative model of the potential outcome is

$$
Y_i(x_i, u_i) = f(x_i, u_i) + \epsilon_i, \ \epsilon_i \sim \mathcal{P}, \mathbb{E}[\epsilon] = 0.
$$

The function $f(x_i,u_i)$ is a function of exposure level $x_i$ and confounder $u_i$, and the random error $\epsilon_i$ is independent of exposure $x_i$ and the confounder $u_i$.

Let $\theta_{(x)} := \mathbb{E}_Y[Y(x)], \ x =0, 1, 2$. The expectations of potential outcome, i.e., the causal effect of $x=0$, is

$$
\theta_{(0)} := \mathbb{E}_Y[Y(0)] = \mathbb{E}_{(Y,U)}[Y(0,u)] = \mathbb{E}_{(\epsilon, U)}[f(0,u) + \epsilon] = \mathbb{E}_U[f(0,u)],
$$

and accordingly, for $x=1,2$,

$$
\theta_{(1)} := \mathbb{E}_Y[Y(1)] = \mathbb{E}_{(Y,U)}[Y(1,u)]= \mathbb{E}_U[f(1, u)], \\
\theta_{(2)} := \mathbb{E}_Y[Y(2)] = \mathbb{E}_{(Y,U)}[Y(2,u)]= \mathbb{E}_U[f(2, u)].
$$

The observed outcome is then one of the potential outcomes:

$$
Y_i := \begin{cases}
Y(0, u_i), \ \text{if } x_i=0; \\
Y(1, u_i), \ \text{if } x_i=1; \\
Y(2, u_i), \ \text{if } x_i=2.
\end{cases}
$$

### 1.2 Propensity score model and inverse probability weighting estimate

Propensity score is defined as the probability of being assigned to the current treatment (exposure) given the confounder. Mathematically,

$$
e_i := e_{x}(u_i) = \operatorname{Pr}(X_i = x | U_i = u_i), \quad x \in \{0, 1, 2\}, \forall i.
$$

Since there are more than 2 levels in $X$, we fit a multinomial logistic regression model to estimate the propensity score:

$$
\operatorname{Pr}(X = x|U=u) = \frac{e^{\beta_x \cdot u}}{\sum_{j=0}^2 e^{\beta_j \cdot u}}, \quad x = 0, 1, 2.
$$

Let $\hat{e}_i$ denote the estimated propensity score of individual $i$. Accordingly, the weight is $\hat{w}_i = 1 / \hat{e}_i$. The modified Horvitz-Thompson estimate of the ATE is:

$$
\hat{\mathbb{E}}[Y(x)] 
= \frac{\sum_{\{i: x_i = x\}} \hat{w}_i Y_i}{\sum_{\{i: x_i = x\}} \hat{w}_i}
= \frac{\sum_{i=1}^n \hat{w}_i \cdot \mathbb{I}[X_i = x] \cdot Y_i}{\sum_{i=1}^n \hat{w}_i \cdot \mathbb{I}[X_i = x]}
, \quad x = 0, 1, 2.
$$

```{r}
set.seed(123123)
n <- 100

# confounder
U <- rnorm(n, mean = 0.5)

# potential outcomes
eps <- rnorm(n)
ffunc <- function(.x, .u) return(.x + .u + .x*.u)
Y2 <- ffunc(2, U) + eps
Y1 <- ffunc(1, U) + eps
Y0 <- ffunc(0, U) + eps
EY2 <- mean(Y2)  ## theoretical: EY2 = 3.5
EY1 <- mean(Y1)  ## theoretical: EY1 = 2
EY0 <- mean(Y0)  ## theoretical: EY0 = 0.5

# treatment assignment: multinomial logistic regression (softmax)
## .sls: list of linear predictors (s). It should contain (K-1) lists, and the element of each list is an n-vector of the linear predictors of the k-th level (1 <= k < K)
softmax <- function(.sls) {
  K <- length(.sls) + 1  ## number of levels
  
  exps.mat <- sapply(.sls, exp)  ## exp(LinearPredictor), n by (K-1)
  Z <- rowSums(exps.mat) + 1  ## denominator
  
  prob.mat <- cbind(1, exps.mat) / Z
  return(prob.mat)
}

xi.ls <- list(rnorm(2), rnorm(2, mean = -0.1))
LinearPred.ls <- lapply(xi.ls, function(.xi) .xi[1] + .xi[2]*U)
probs <- softmax(LinearPred.ls)
X <- t(apply(probs, 1, function(pr) {rmultinom(1, 1, prob = pr)})) ## n by K

# observed outcome
Y <- X[,(0+1)] * Y0 + X[,(1+1)] * Y1 + X[,(2+1)] * Y2
```

```{r}
# Inverse Probability Weighting

## Propensity score model: multinomial logistic regression model
library(nnet)
mn.fit <- multinom(X ~ U)
ProbPred <- predict(mn.fit, type = "probs")

# par(mfrow=c(1,3))
# for (j in 1 : 3) {
#   plot(probs[,j], ProbPred[,j], 
#        xlab = paste( "True prob of class", (j-1) ),
#        ylab = paste( "Predicted prob of class", (j-1)) )
#   abline(a=0, b=1)
# }

probs.obs <- rowSums(X * probs)
ProbPred.obs <- rowSums(X * ProbPred)
plot(probs.obs, ProbPred.obs, 
     xlab = "True prob of observed class",
     ylab = "Predicted prob of observed class")
abline(a=0, b=1)

W <- 1 / ProbPred.obs
estEY0 <- sum(W * X[,(0+1)] * Y) / sum(W * X[,(0+1)])
estEY1 <- sum(W * X[,(1+1)] * Y) / sum(W * X[,(1+1)])
estEY2 <- sum(W * X[,(2+1)] * Y) / sum(W * X[,(2+1)])
```

```{r}
#| message: false
#| echo: false

ind <- 1 : n

arrow_df <- data.frame(Y = Y[ind], U = U[ind], 
                       Y0 = Y0[ind], Y1 = Y1[ind], Y2 = Y2[ind]) %>%
  mutate(X = as.factor( apply( X[ind,], 1, 
                               function(.r) .r %*% as.matrix(0:2) )) )
point_df <- arrow_df %>% 
  mutate(W = W[ind]) %>%
  pivot_longer(cols = Y0:Y2, 
               values_to = "PO_val", names_to = "PO_name") %>%
  mutate(Y_PO_name = paste0("Y", X))

ggplot() +
  geom_segment(data = arrow_df,
               aes(x = U, xend = U, y = Y0, yend = Y2),
               color = "gray", alpha = 0.2) +
  geom_point(data = point_df, 
             aes(x = U, y = PO_val, color = PO_name), 
             size = 2, alpha = 0.3) + 
  geom_point(data = point_df, 
             aes(x = U, y = Y, color = Y_PO_name), size = 3) +
  labs(x = "Confounder", y = "Outcome", color = "Exposure", 
       title = "What we want to see:") + 
  scale_color_discrete(labels = c("0", "1", "2")) + 
  theme_light()
```

```{r, echo=F}
ggplot() +
  geom_point(data = point_df, aes(x = U, y = Y, color = X), size = 3) +
  labs(x = "Confounder", y = "Outcome", color = "Exposure", 
       title = "What we observe:") + 
  scale_colour_discrete(labels = c("0", "1", "2")) + 
  theme_light()
```

```{r}
#| echo: false

line_df <- data.frame(X = as.factor(c("0", "1", "2")), 
                      EY = c(EY0, EY1, EY2), 
                      estEY = c(estEY0, estEY1, estEY2)) %>%
  pivot_longer(cols = EY:estEY, names_to = "class") %>%
  mutate(class = as.factor(class))

ggplot() +
  geom_hline(data = line_df, aes(yintercept = value, color = X, 
                                 linetype = fct_rev(class)),
             alpha = 0.8) + 
  geom_point(data = point_df, aes(x = U, y = Y, color = X, 
                                  size = W)) +
  labs(x = "Confounder", y = "Observed outcome", color = "Exposure", 
       linetype = "", size = "Weight", 
       title = "Weighted by inverse of propensity score:") + 
  scale_size(range = c(1, 5)) +
  scale_colour_discrete(labels = c("0", "1", "2")) + 
  scale_linetype_discrete(labels = c("Mean EpY", "IPW Estimate")) +
  theme_light()
```

------------------------------------------------------------------------

## 2. Identification assumptions for multiple exposures

> [Wang, Yixin, and David M. Blei. "The blessings of multiple causes." Journal of the American Statistical Association 114.528 (2019): 1574-1596.](https://arxiv.org/pdf/1805.06826)

The definition of the (additive) potential outcome model is given by:

$$
Y_i(\mathbf{x}_i, u_i) = \sum_{j=1}^p f_j(x_{ij}, u_i) + \epsilon_i, \epsilon_i \sim \mathcal{P}, \mathbb{E}[\epsilon] = 0,
$$

where $x_{ij} \in \{0,1,2\} \ (\forall i,j)$, $f_j(x_j, u)$ is the joint effect of $X_j$ and $U$.

We use the same identification assumptions.

**Definition (Weak unconfoundedness) (Imbens 2020)**. The assigned causes are weakly unconfounded given $U_i$ if

$$
(X_{i1}, \ldots, X_{ip}) \perp\!\!\perp Y_i(\mathbf{x}) | U_i
$$

for all $(x_1, \ldots,x_p) \in \mathcal{X}_1 \otimes \cdots \otimes \mathcal{X}_p$, and $i=1, \ldots, n$.

**Definition (No unobserved single-cause confounders)**. Denote $V_i$ are the observed covariates. There are no unobserved single-cause confounders for the assigned exposures $X_{i1}, \ldots, X_{ip}$ if, for $j=1, \ldots, p$,

1.  There exist some random variable $U_{ij}$ such that

    $$
    X_{ij} \perp Y_i(\mathbf{x}) | V_i, U_{ij}, \\
    X_{ij} \perp X_{i, -j} | U_{ij},
    $$

    where $X_{i, -j} = \{X_{i1}, \ldots, X_{ip}\} \backslash X_{ij}$ is the complete set of $p$ exposures excluding the $j$th exposure;

2.  There exists no proper subset of the sigma algebra $\sigma(U_{ij})$ satisfies the second equation in 1.

That is, $U_{ij}$ refers to the multiple-cause confounders that affect the $j$th exposure $X_{ij}$.

...

**Theorem (Identification of the average causal effect of all the exposures)**. Assume SUTVA, no unobserved single-cause confounders, and consistency of substitute confounders...

The conditions that make this theorem holds seem inappropriate to our method. (??!)

This is because our model definition is different than Wang's. We assume the exposure effects are additive, but the relationship between each exposure and the confounder are assumed nonparametrically.

*Double check needed*.

------------------------------------------------------------------------

## 3. How to quantify the causal effects?

Assume $(X_{i1}, \ldots, X_{ip})$ are the exposure assignment of the $i$th unit.

Suppose the identification assumptions hold. Specifically, the no unobserved confounder assumption is: given the confounder (or confounders) $U_i$, we assume

$$
(X_{i1}, \ldots, X_{ip}) \perp\!\!\perp Y_i(\mathbf{x}) | U_i, (??!) \\
X_{ij} \perp\!\!\perp X_{i, -j} | U_i. 
$$

The definition of the (additive) potential outcome model is given by:

$$
Y_i(\mathbf{x}_i, u_i) = \sum_{j=1}^p f_j(x_{ij}, u_i) + \epsilon_i, \epsilon_i \overset{\text{ind.}}{\sim} \mathcal{P}, \mathbb{E}[\epsilon] = 0,
$$

where $x_{ij} \in \{0,1,2\} \ (\forall i,j)$, $f_j(x, u)$ is the joint effect of the $j$th exposure $X_j$ and the confounder $U$, and the random error $\epsilon_i$ is independent of $(X_{i1}, \ldots, X_{ip}, U_i)$.

According to the identification assumptions, the average causal effect $\mathbb{E}_Y[Y(\mathbf{x})]$ for $\mathbf{x}=(x_1, \ldots, x_p)$ can be computed by

$$
\mathbb{E}_Y[Y(\mathbf{x})] = \mathbb{E}_{(Y,U)}[Y(\mathbf{x}, u)] \\
= \mathbb{E}_U \{ \mathbb{E}_Y[Y(\mathbf{x}, u)] \} \\
= \mathbb{E}_U \left\{ \mathbb{E}_{\epsilon} \left[\sum_{j=1}^p f_j (x_j, u) + \epsilon \right] \right\} \\
= \sum_{j=1}^p\mathbb{E}_U \left[ f_j(x_j, u)\right].
$$

Denote $\theta^{(x)}_{j} := \mathbb{E}_U[f_j(x, u)]$ be the expectation of the causal effect of the $j$th exposure. Then the equations above can be simplified as

$$
\mathbb{E}_Y[Y(\mathbf{x})] = \sum_{j=1}^p\theta_j^{(x_j)} = \sum_{j=1}^p \left\{ \theta_j^{(0)} \cdot \mathbb{I}[x_j=0] + \theta_j^{(1)} \cdot \mathbb{I}[x_j=1] + \theta_j^{(2)} \cdot \mathbb{I}[x_j=2] \right\}.
$$In other words, $\theta_j^{(x)}$ is regarded as the expectation of marginal causal effect of the $j$th exposure.

### 3.2 Encoding categorical exposures

Suppose $X_j \in \{0,1,2\}$.

#### Version 1: "tranditional" dummy variables

For a 3-level categorical variable, we create 2 dummy variables:

$$
X_j \Rightarrow \left[X_{j(1)}, \ X_{j(2)} \right], \\
X_{j(1)} = \begin{cases} 1, \text{ if } X_j = 1 \\ 0, \text{ otherwise}  \end{cases}, \quad 
X_{j(2)} = \begin{cases} 1, \text{ if } X_j = 2 \\ 0, \text{ otherwise}  \end{cases},
$$

To be more specific,

$$
X_j = 0 \Rightarrow [0, \ 0]; \\
X_j = 1 \Rightarrow [1, \ 0]; \\
X_j = 2 \Rightarrow [0, \ 1].
$$

Moreover, denote $\Delta_{j(1)} = \theta_{j(1)} - \theta_{j(0)}$ and $\Delta_{j(2)} = \theta_{j(2)} - \theta_{j(0)}$, which represent the average treatment effects of $X_j=1$ and $X_j=2$ versus $X_j=0$, respectively. The average causal effect equation in Section 3 is equivalent to:

$$
\mathbb{E}_Y[Y(\mathbf{x})] = \sum_{j=1}^p \left\{ \theta_{j(0)} + \Delta_{j(1)}\cdot X_j^{(1)} + \Delta_{j(2)} \cdot X_j^{(2)} \right\}.
$$

After all, to estimate $\left(\Delta_{j(1)}, \Delta_{j(2)} \right)$ , we need to estimate $\left(\theta_{j(0)}, \theta_{j(1)}, \theta_{j(2)}\right)$. Then it turns to our second encoding version: one-hot encoding.

#### Version 2: one-hot encoding

For a 3-level categorical variable, we create three indicators to represent the level of $X_j$:

$$
X_j \Rightarrow \left[X_j^{(0)}, \ X_j^{(1)}, \ X_j^{(2)} \right], \\
X_j^{(0)} = \begin{cases} 1, \text{ if } X_j = 0 \\ 0, \text{ otherwise}  \end{cases}, \quad 
X_j^{(1)} = \begin{cases} 1, \text{ if } X_j = 1 \\ 0, \text{ otherwise}  \end{cases}, \quad 
X_j^{(2)} = \begin{cases} 1, \text{ if } X_j = 2 \\ 0, \text{ otherwise}  \end{cases},
$$

To be more specific,

$$
X_j = 0 \Rightarrow [1, \ 0, \ 0]; \\
X_j = 1 \Rightarrow [0, \ 1, \ 0]; \\
X_j = 2 \Rightarrow [0, \ 0, \ 1].
$$

Still use $\Delta_{j(1)}$ and $\Delta_{j(2)}$ to denote the average treatment effects. Then the average causal effect equation is equivalent to (just as the equation in the intro paragraphs of Section 3?):

$$
\mathbb{E}_Y[Y(\mathbf{x})] = \sum_{j=1}^p \left\{ \theta_{j(0)} \cdot X_j^{(0)} + \theta_{j(1)} \cdot X_j^{(1)} + \theta_{j(2)} \cdot X_j^{(2)} \right\}.
$$

Then, $\Delta_{j(1)} = \theta_{j(1)} - \theta_{j(0)}$ and $\Delta_{j(2)} = \theta_{j(2)} - \theta_{j(0)}$.

#### Interpretation of $\Delta_{j(1)}$ and $\Delta_{j(2)}$

-   The y-axis does not reflect the true effect; it only represents the **relative relationships** between the three points.)

-   $(\Delta_{j(1)}, \Delta_{j(2)})$ are the differences of points in y-axis! Not the slopes of lines between the two points)

1.  If $X_j$ has an additive effect, then $\Delta_{j(1)}$ and $\Delta_{j(2)}$ are be significant and $\Delta_{j(1)} \cdot \Delta_{j(2)} > 0$ and $\left|\Delta_{j(1)} \right| < \left| \Delta_{j(2)} \right|$.

```{r}
#| echo: false
 
data.frame(x = 0:2, 
           y = 0:2) %>%
  ggplot(aes(x = x, y = y)) + 
  geom_point(size = 3) + 
  geom_line(linetype = 2) + 
  labs(x = "Exposure (X)", y = "Effect (theta)", 
       title = "Additive effect") + 
  theme_minimal()
```

2.  If $X_j$ has an xor-like effect (reciprocal sign epistasis ?), then at least $\Delta_{j(1)}$ is significant and $\left| \Delta_{j(1)} \right| > \left|\Delta_{j(2)} \right|$.

```{r}
#| echo: false

data.frame(x = 0:2, 
           y = c(0, 1, 0)) %>%
  ggplot(aes(x = x, y = y)) + 
  geom_point(size = 3) + 
  geom_line(linetype = 2) + 
  labs(x = "Exposure (X)", y = "Effect (theta)", 
       title = "XOR effect") + 
  theme_minimal()
```

3.  If $X_j$ has a dominant effect, then $\Delta_{j(1)}$ and $\Delta_{j(2)}$ are significant and $\Delta_{j(1)} = \Delta_{j(2)}$.

```{r}
#| echo: false

data.frame(x = 0:2, y = c(0, 1, 1)) %>%
  ggplot(aes(x = x, y = y)) + 
  geom_point(size = 3) + 
  geom_line(linetype = 2) + 
  labs(x = "Exposure (X)", y = "Effect (theta)", 
       title = "Dominant effect") + 
  theme_minimal()
```

4.  If $X_j$ is a null variable, then $\Delta_{j(1)}$ and $\Delta_{j(2)}$ are insignificant.

```{r}
#| echo: false

data.frame(x = 0:2, y = rep(0, times = 3)) %>%
  ggplot(aes(x = x, y = y)) + 
  geom_point(size = 3) + 
  geom_line(linetype = 2) + 
  labs(x = "Exposure (X)", y = "Effect (theta)", 
       title = "Null effect") + 
  theme_minimal()
```

In short, when identifying what type of effect $X_j$ has on the response, the baseline effect $\theta_{j(0)}$ is trivial; what matters are the differences, i.e., $\Delta_{j(1)}$ and $\Delta_{j(2)}$. If $X_j$ is not a null variable, then at least one of $\Delta_{j(1)}$ and $\Delta_{j(2)}$ is significant.

------------------------------------------------------------------------

## 4. Causal SuSiE for multiple categorical exposures

Following the additive potential outcome model defined above:

$$
Y_i(\mathbf{x}_i) = \sum_{j=1}^p \left(\theta_{j(0)} + \Delta_{j(1)} X_{ij}^{(1)} + \Delta_{j(2)} X_{ij}^{(2)} \right) + \epsilon_i, 
$$

We use **version 1** encoding: two dummy variables representing one 3-level categorical exposures.

In our case, $\left\{\theta_{j(0)} \right\}_{j=1}^p$ are identifiable via inverse probability weighting. (? is it?) Meanwhile, the value of $\theta_{j(0)}$ does not affect the significance of $X_j$.

### 4.1 Variational inference

Based on the interpretation of $\Delta_{j(1)}$ and $\Delta_{j(2)}$ , we concentrate on these two coefficients.

#### The estimator and the corresponding SDs of $\Delta_{(k)}$

Suppose there is only one $X$ (p=1). We consider the simple model as follows:

$$
Y_i(x_i) = \left(\theta_{(0)} + \Delta_{(1)} X_{ij}^{(1)} + \Delta_{(2)} X_{ij}^{(2)} \right) + \epsilon_i. 
$$

Take $\Delta_{(1)}$ as an example. We first obtain the estimates of $\left(\theta_{(0)},\theta_{(1)} \right)$ by the IPW estimator:

$$
\hat{\Delta}_{(1)} = \hat{\theta}_{(1)} - \hat{\theta}_{(0)},
$$

The corresponding **variance** can be approximated by the following two ways:

\(1\) **bootstrap variance** of $\tilde{\Delta}_B^{(1)} = \tilde{\theta}_{\text{B}}^{(1)} - \tilde{\theta}_{\text{B}}^{(0)}$, denoted as $s^2(\Delta_{(1)})$.

\(2\) the **variance formula** from the **two-sample t-test with unequal variance**: denote the bootstrap variance of $\hat{\theta}^{(1)}$ and $\hat{\theta}^{(0)}$ as $s^2(\theta^{(1)})$ and $s^2(\theta^{(0)})$, then the pooled variance formula is:

$$
s^2(\Delta_{(1)}) = \frac{s^{2}(\theta_{(0)})}{n_0} + \frac{s^{2}(\theta_{(1)})}{n_1}, 
$$

where $n_0 := \# \{i: X_i =0\}$ and $n_1 := \# \{i: X_i = 1 \}$ are the sample sizes of each group.

Here are the assumptions of two-sample t-tests:

-   Data values must be independent. Measurements for one observation do not affect measurements for any other observation.

-   Data in each group must be obtained via a random sample from the population.

-   Data in each group are **normally distributed**.

-   Data values are continuous.

That is, distributional assumptions are required if applying approach (2).

Approach (1), on the contrary, is completely nonparametric. It is the most direct but conservative way to approximate the variance of estimate of **ATE**, the **difference** between the two average causal effect estimates. I prefer the this approach.

```{r}
## Compare the two standard deviation estimates
set.seed(234)
nBoots <- 200
thetahat <- matrix(nrow = nBoots, ncol = 3)
for (B in 1 : nBoots) {
  boot.ind <- sample.int(nrow(X), size = nrow(X), replace = T)
  Xboot <- X[boot.ind, , drop=F]
  Yboot <- Y[boot.ind]
  Wboot <- W[boot.ind]
  
 thetahat[B,] <- colSums(sweep(Xboot, 1, Wboot*Yboot, "*")) / colSums(sweep(Xboot, 1, Wboot, "*"))
}

## (1) Bootstrap variance of ATE (direct). 
deltahat <- cbind(
  delta1 = thetahat[,(1+1)] - thetahat[,(0+1)],
  delta2 = thetahat[,(2+1)] - thetahat[,(0+1)]
)
deltahat_var_v1 <- apply(deltahat, 2, var)
deltahat_var_v1

zscore_v1 <- c(
  (estEY1 - estEY0) / sqrt(deltahat_var_v1["delta1"]),
  (estEY2 - estEY0) / sqrt(deltahat_var_v1["delta2"])
)
print("zscore:")
zscore_v1

## (2) two sample t-test with unequal variance assumption
thetahat_var <- apply(thetahat, 2, var)
nsamples <- colSums(X)

deltahat_var_v2 <- c(
  delta1 = thetahat_var[(0+1)] / nsamples[(0+1)] + thetahat_var[(1+1)] / nsamples[(1+1)],
  delta2 = thetahat_var[(0+1)] / nsamples[(0+1)] + thetahat_var[(2+1)] / nsamples[(2+1)]
)
deltahat_var_v2

tscore_v2 <- c(
  (estEY1 - estEY0) / sqrt(deltahat_var_v2["delta1"]),
  (estEY2 - estEY0) / sqrt(deltahat_var_v2["delta2"])
)
print("tscore:")
tscore_v2
```

#### Continue: simple outcome model with one categorical variable

The assigned prior distribution of $\Delta_{(k)}, k=1,2$ is:

$$
\Delta_{(k)} \sim N \left(0, \sigma_{0}^2 (\Delta_{(k)}) \right).
$$

Then the posterior distribution of $\Delta_{(k)}$ is :

$$
\Delta_{(k)} | \mathbf{y}, \mathbf{x}, \hat{\mathbf{w}}, \sigma^2, \sigma^2_0 \sim \mathcal{N} \left(\mu_1(\Delta_{(k)}), \sigma_1^{2}(\Delta_{(k)}) \right), 
$$

where

$$
\sigma_1^2(\Delta_{(k)}) := \sigma_1^{2}(\mathbf{x, y, \hat{w}}; \sigma^2, \sigma^2_0)(\Delta_{(k)}) := \frac{1}{1/s^{2}(\Delta_{(k)}) + 1/\sigma_{0}^2 (\Delta_{(k)})}, \\
\mu_1(\Delta_{(k)}) := \mu_1(\mathbf{x, y, \hat{w}}; \sigma^2, \sigma^2_0)(\Delta_{(k)}) := \frac{\sigma^2_1(\Delta_{(k)})}{s^2(\Delta_{(k)})} \times \hat{\Delta}_{(k)}. 
$$

Just as the solutions of the previous steps. Accordingly, the approximate Bayes Factor (ABF) for whether $\Delta_{(k)}=0$ is

$$
\operatorname{BF}(\Delta_{(k)}) = \sqrt{ \frac{s^2}{\sigma_0^2 + s^2} } \exp \left( \frac{z^2}{2} \times \frac{\sigma^2_0}{\sigma_0^2 + s^2} \right),
$$

where each notation corresponds to its counterpart of $\Delta_{(k)}$, and $z^2 = \hat{\Delta}^2 / s^2$ is the square of z-score.

#### Single-effect regression

Now consider the multiple regression model:

$$
Y_i(\mathbf{x}_i) = \sum_{j=1}^p \left(\theta_{j}^{(0)} + \Delta_j^{(1)} X_{ij}^{(1)} + \Delta_j^{(2)} X_{ij}^{(2)} \right) + \epsilon_i  \\
= \sum_{j=1}^p \left( \Delta^{(1)}_j X_{ij}^{(1)} + \Delta_j^{(2)} X_{ij}^{(2)} \right) + \sum_{j=1}^p \theta_j^{(0)} + \epsilon_i.
$$

Assume that each layer $l$ captures only one $\Delta_{j(k)}, \ k=1,2$. Let $\boldsymbol{\Delta} =(\Delta_{1(1)}, \Delta_{1(2)}, \ldots, \Delta_{p(1)}, \Delta_{p(2)}) \in \mathbb{R}^{2p}$. The prior distribution assigned to $\boldsymbol{\Delta}$ is:

$$
\boldsymbol{\Delta} = \delta \boldsymbol{\gamma}, \\
\delta \sim \mathcal{N}(0, \sigma_0^2), \\ 
\boldsymbol{\gamma} \sim \operatorname{Mult}(1, \boldsymbol{\pi}). 
$$

Then, accordingly, the posterior distribution of $\boldsymbol{\Delta}$ is given by

$$
\boldsymbol{\gamma}| \mathbf{X, y, \hat{W}}, \sigma^2, \sigma^2_0 \sim \operatorname{Mult}(1, \boldsymbol{\alpha}), \\
\delta| \mathbf{X, y, \hat{W}}, \sigma^2, \sigma^2_0, \gamma_{j(k)} = 1 \sim \mathcal{N}(\mu_{1,j(k)}, \sigma_{1,j(k)}^2), 
$$

where $\boldsymbol{\alpha}$ is the vector of posterior inclusion probabilities (PIPs) ...

***Messy notations....***
