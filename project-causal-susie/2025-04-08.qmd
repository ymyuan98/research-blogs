---
title: "8 April 2025"
format: 
  gfm:
    html-math-method: webtex
editor: visual
bibliography: references.bib
---

## Recall ...

Here, we discuss the following (additive) potential outcome model.

$$
Y_i(\mathbf{x}_i, u_i) = \sum_{j=1}^p f_j(x_{ij}, u_i) + \epsilon_i, \epsilon_i \sim \mathcal{N}(0,\sigma^2), 
$$

where $x_{ij} \in \{0,1,2\} \ (\forall i,j)$, $f_j(x_j, u)$ is the joint effect of $X_j$ and $U$.

Assume the causal effects are identifiable under certain assumptions.

As we proposed before, we can use approaches like IPW to estimate the causal effect of variable $X_j$ (for all $j$), and then the potential outcome model can be approximately expressed as

$$
Y_i(\mathbf{x}_i) = \sum_{j=1}^p \left(\theta_{j(0)} + \Delta_{j(1)} X_{ij}^{(1)} + \Delta_{j(2)} X_{ij}^{(2)} \right) + \epsilon_i, \ \epsilon_i \sim \mathcal{N}(0, \sigma^2),
$$

where, for $j=1, \ldots, p$, $\theta_{j(k)} := \mathbb{E}_U[f_j(k, u)]$ are the marginal average causal effect of $X_j$ when $X_j=k \ (k=0,1,2)$, $\Delta_{j(k)} = \theta_{j(k)} - \theta_{j(0)}, \ (k=1,2)$ are the average treatment effects, and $X_{ij}^{(k)} := \mathbb{I}[X_{ij} = k], \ (k=1,2)$ are the dummy variables of $X_j$ of the $i$th individual.

------------------------------------------------------------------------

## 1. Causal single-effect regression: select categorical exposure $X_j$ by the "joint significance" of $\Delta_{j(k)}$'s

As shown in Section 3 of `2025-03-17.qmd`, $X_j$ is considered a causal variable only when either $\Delta_{j(1)}$ or $\Delta_{j(2)}$ is nonzero. That is,

$$
\operatorname{Pr}\{X_j \text{ is causal}|\text{data}\} = \operatorname{Pr}\{\Delta_{j(1)} \neq 0 \text{ or } \Delta_{j(2)} \neq 0 |\text{data} \} = \operatorname{Pr}\{\gamma_{j(1)} = 1 \text{ or } \gamma_{j(2)} = 1  |\text{data}\}
$$

The events $\{\gamma_{j(1)} = 1\}$ and $\{ \gamma_{j(2)}=1 \}$ are **mutually exclusive**, i.e., $\{\gamma_{j(1)} = 1\}$ and $\{\gamma_{j(2)} = 1\}$ will never happen at the same time according to the definition of $\boldsymbol{\gamma}$. Therefore,

$$
\operatorname{Pr}(\gamma_{j(1)} = 1 \text{ or } \gamma_{j(2)} = 1 |\text{data}) = \operatorname{Pr}(\gamma_{j(1)} = 1 |\text{data}) + \operatorname{Pr}(\gamma_{j(2)} = 1 |\text{data}) = \alpha_{j(1)} + \alpha_{j(2)}.
$$

To sum up, for a categorical variable $X_j$ that has $K_j$ variables, the probability of $X_j$ being causal (?), namely variable-wise posterior inclusion probability (varPIP), is:

$$
\alpha_j := \operatorname{Pr}(X_j \text{ is causal}| \mathbf{X,y,W}) = \sum_{k=1}^{K_j-1} \alpha_{j(k)}. 
$$ {#eq-varPIP}

Similarly, we call $\alpha_{j(k)}\ (k=1, \ldots, K_j-1)$ the level-wise posterior inclusion probability (levPIP) of the $j$th variable.

### 1.1 A simple case: binary $X_j$ 

Then, the level-wise PIP equals the variable-wise PIP, i.e.,

$$
\operatorname{Pr}(X_j \text{ is causal} |\text{data}) = \alpha_{j(1)} =: \alpha_j. 
$$

------------------------------------------------------------------------

## 2. Identification of treatment effects (?)

It seems that the baseline effect estimations are usually inaccurate, but the estimation of treatment effects (i.e., the differences) are relatively more stable and accurate.

If there is only one causal variable, then it is possible that we may have an accurate baseline effect estimate for the causal variables. However, when there are more than one causal variables, it is very likely that the baseline effect estimate for all variables are inaccurate: the differences between the estimates and the true values are quite big.

Review the identification assumptions of [@wang2019]:

### 2.1 Confounders and Unconfoundedness

**Definition 1 (weak unconfoundedness, (Imbens 2000))**. The assigned causes are weakly unconfounded given $Z_i$ if

$$
(X_{i1}, \ldots, X_{ip}) \perp\!\!\perp Y_i(\mathbf{x})| Z_i
$$

for all $(x_{1}, \ldots, x_{p}) \in \mathcal{X}_1 \otimes \cdots \otimes \mathcal{X}_p$ and $i=1, \ldots, n$.

> "The assigned causes are weakly confounded if all confounders are measurable with respect to the $\sigma$-algebra generated by $Z_i$."

**Definition 2 (No unobserved single-cause confounders)**. Denote $V_i$ are the observed covariates. There are no unobserved single-cause confounders for the assigned exposures (causes) $X_{i1}, \ldots, X_{ip}$ if, for $j=1, \ldots, p$,

1.  There exist some random variable $U_{ij}$ such that

    $$
    X_{ij} \perp Y_i(\mathbf{x}) | V_i, U_{ij}, \\
    X_{ij} \perp X_{i, -j} | U_{ij},
    $$ {#eq-def2-1}

    where $X_{i, -j} = \{X_{i1}, \ldots, X_{ip}\} \backslash X_{ij}$ is the complete set of $p$ exposures excluding the $j$-th exposure;

2.  There exists no proper subset of the sigma algebra $\sigma(U_{ij})$ satisfies the second equation in @eq-def2-1.

> -   $U_{ij}$ refers to the multiple-cause confounder that affect the $j$th exposure $X_{ij}$, as it induces the dependence between $X_{ij}$ and the potential outcome $Y_i(\mathbf{x})$ and that between $X_{ij}$ and all the other exposures $X_{i, -j}$.
>
> -   The observed covariate $V_i$ refers to the single-cause confounders since it only affect the relationship between $X_{ij}$ and $Y_i(\mathbf{x})$. It can be a vector of observed covariates.
>
> -   "By the first equation of @eq-def2-1, $(V_i, U_{ij})$ satisfy unconfoundedness."
>
> -   "The second equation of @eq-def2-1 guarantees that $U_{ij}$ can be **recovered by constructing a random variable** $Z_i$ **that renders all the causes conditionally independent.**"

### 2.2 Causal Identification of (Wang et al. 2020)

**Definition 3 (Consistency of substitute confounders)**. The factor model $p(\xi, z, \mathbf{x})$ admits consistent estimates of the substitute confounder $U_i$ if, for some function $g_{\xi}$,

$$
p(z_i|\mathbf{x}_i, \xi) = \delta_{g_{\xi}(\mathbf{x}_i)}. 
$$

> "It requires that we can estimate the substitute confounder $Z_i$ from the causes $\boldsymbol{X}_i$ with certainty."

**Theorem 4 (Identification of the average causal effect of all the causes).** Assume SUTVA, no unobserved single-cause confounders, and consistency of substitute confounders. Then, under conditions described below, the deconfounder (their proposed method) nonparametrically identifies the average treatment effect of all the causes. The average treatment effect of changing the causes from $\mathbf{x} = (x_1, \ldots, x_p)$ to $\mathbf{x}' = (x_1', \ldots, x_p')$ is

$$
\mathbb{E}_Y[Y(\mathbf{x})] - \mathbb{E}_Y[Y(\mathbf{x}')] = \mathbb{E}_{Z,V}[\mathbb{E}_Y[Y_i|\mathbf{X}_i=\mathbf{x}_i, Z_i, V_i]] - \mathbb{E}_{Z,V}[\mathbb{E}_Y[Y_i|\mathbf{X}_i=\mathbf{x}'_i, Z_i, V_i]].
$$

This holds with the following two conditions: (1) the substitute confounder is a piece-wise constant function of the (continuous) causes: $\nabla_\mathbf{x} g_{\xi}(\mathbf{x})=0$ up to a set of Lebesgue measure zero; (2) the outcome is **separable**,

$$
\mathbb{E}[Y_i(\mathbf{x})|Z_i = z, V_i = v] = h_1(\mathbf{x}_i, v) + h_2(z), \\
\mathbb{E}[Y_i|\mathbf{X}_i = \mathbf{x}, Z_i = z, V_i = v] = h_3(\mathbf{x}_i, v) + h_4(z), 
$$

for all $(\mathbf{x}, v,u) \in \mathcal{X} \times \mathcal{V} \times \mathcal{U}$ and some continuously differentiable functions $h_1, h_2, h_3$, and $h_4$.

> The separable condition implies that the multiple-cause confounder $Z$ does not interact with the causes $\boldsymbol{X}$.
>
> This conditionseem **inappropriate** in our model because we assume that the multiple confounder $U$ can interact with the causes $\boldsymbol{X}$.

**Theorem 5 (Identification of the average causal effect of subsets of the causes)**. Assume SUTVA, no unobserved single-cause confounders, and consistency of substitute confounders. Then, under the condition descrived below, the deconfounder (?) nonparametrically identifies the average causal effect of subsets of causes. The average causal effect of changing the first $m (m<p)$ causes from $x_{1:m}=(x_1, \ldots, x_m)$ to $x_{1:m}' = (x_1', \ldots, x_m')$ is

$$
\mathbb{E}_{X_{(m+1):p}}[\mathbb{E}_Y[Y_i(x_{1:m}, X_{i,(m+1):p})]] -
\mathbb{E}_{X_{(m+1):p}}[\mathbb{E}_Y[Y_i(x_{1:m}', X_{i,(m+1):p})]] \\
= \mathbb{E}_{Z,U}[\mathbb{E}_Y[Y_i|Z_i, V_i, X_{i,1:m}=x_{1:m}]] - \mathbb{E}_{Z,U}[\mathbb{E}_Y[Y_i|Z_i, V_i, X_{i,1:m}=x_{1:m}']].
$$

This holds with the following condition: the first $k$ causes $X_{i1}, \ldots, X_{im}$ satisfy overlap, $P((X_{i1}, \ldots, X_{im}) \in \mathcal{X}|Z_i, V_i) > 0$ for any set $\mathcal{A}$ such that $P(\mathcal{X}) > 0$.

### 2.3. Is it possible to identify the baseline effects without bias where more than two causal variables exist? 

Maybe no..

------------------------------------------------------------------------

## 3. What would be the "residuals" after removing the effect of the $l$-th layer? 

To define the residuals, we first need to confirm that what each layer $L$, estimated by function `Causal-SER` , represents.

Here are a two interpretations:

### 3.1 Treat each level independently in each layer, i.e., traditional way. 

As usual, each layer $l$ is designed to estimate a non-zero treatment effect $\Delta_{j(k)}$. In this case, each $\Delta_{j(k)}$ is treated independently and, for the $j$th exposure, the effect of the $l$-th layer is $\sum_{k=1}^{K_j-1} X_{j(k)} \cdot (\alpha_{j(k)} \mu_{j(k)})$.

![](images/clipboard-831464435.png)

### 3.2 Group the levels of each variable and propagate the corresponding variable-wise PIP to each level.

(Hierarchical PIP) The output of `Causal-SER` of layer $l$ represents the $l$-th significant (?) causal effect estimates. Hence, for each $j$th exposure $X_j  \rightarrow (X_{j}^{(0)}, X_{j}^{(1)}, \ldots, X_{j}^{(K_j-1)})$, the posterior estimates of $(\Delta_{j(1)}, \ldots, \Delta_{j(K_j-1)})$ should be considered as a whole.

1.  In this case, we define the probability of $X_j$ being a causal variable as $\alpha_j$ by @eq-varPIP, and the varPIP is propogated to all posterior mean estimates of $(\Delta_{j(1)}, \ldots, \Delta_{j(K_j-1)})$, i.e., $(\mu_{j(1)}, \ldots, \mu_{j(K_j-1)})$. Then, the effect of the $l$-th layer is $(X_{j(1)}, \ldots, X_{j(K_j-1)})(\boldsymbol{\alpha}_j \odot (\mu_{j(1)}, \ldots, \mu_{j(K_j-1)}))^\top$.

![](images/clipboard-622317123.png)

However, unfortunately, this method shows that the effect size will be overestimated and the expected log-likelihood and the ELBO will decrease. (Weird, weird).

Will remove this option.
