---
title: "5 May 2025"
subtitle: "Potential alternative solutions to the overlap and class imbalance problem"
format: 
  gfm:
    html-math-method: webtex
editor: visual
bibliography: references.bib
---

## Recall ...

(Copied) The biggest challenge up to now is that, when treating each variant as a categorical variable, class imbalanced is inevitable: for some exposure variable $X$, many objects are with $X=0$, whereas only a few objects are with $X=1$ or $X=2$.

The class imbalance brings trouble to estimating the propensity scores, i.e., $\operatorname{Pr}(X=k|U), k=0, 1, 2$ as some estimated propensity scores are close to or even equal to either 0 and 1. This would lead to the situation that the IPW estimator would be determined by

------------------------------------------------------------------------

## Challenge 1: infer a valid substitute confounder

In the previous simulation studies, we observed that, when using the principal component analysis (PCA) to infer the substitute confounder, we noticed that some confounder could perfectly separate the level of $X$, not to mention that the estimated propensity scores have **low overlap** among the levels.

@wang2019 is a reference. In Section 4.1.1, they suggest that "when the causes are counts, **Poisson factorization (PF)** is an appropriate factor model."

Let $Z$ be the substitute confounder.

Copied from @wang2019: PF is a probabilistic form of nonnegative matrix factorization, where $z_i$ and $\theta_j$ are positive $K$-vectors,

$$
Z_{ik} \sim \operatorname{Gamma}(\alpha_0, \alpha_1), \quad k = 1, \cdots, K, \\
X_{ij} | Z_i \sim \operatorname{Poisson}(z_i^\top \theta_j), \quad j = 1, \ldots, p.
$$

Suppose we have a set of potential substitute confounders. The next step suggested is **predictive checks for the assignment model**.

... Relatively complicated.

@wang2019 assessed a couple of factorization approaches to see if they could infer valid substitute confounders.

------------------------------------------------------------------------

## Challenge 2: address the individuals with extreme weights

> [Matsouaka, Roland A., and Yunji Zhou. "Causal inference in the absence of positivity: The role of overlap weights." *Biometrical Journal* 66.4 (2024): 2300156.](https://pubmed.ncbi.nlm.nih.gov/38847059/)

This paper proposed that, when suffering from a lack of adaquate positivity, i.e., violation or near violation of positivity assumption, "there is no point in estimating the ATE for the whole population since such an estimate will be based purely on extrapolating its value(s) in the regions of the covariate distributions without overlap." Therefore, instead of estimating the ATE, they suggested estimating the **weighted average treatment effect (WATE)**,

$$
\tau_g = \frac{E[g(U)\tau(U)]}{E[g(U)]}= \left( \int g(u) f(u) du \right)^{-1} \int \tau(u) f(u) g(u) du,
$$

where $\tau(u) = \mathbb{E}[Y(1) - Y(0)|U=u]$ refers to the conditional average treatment effect (CATE), and $g(u)$ refers to the selection function, and can be defined as a function of the propensity score $e(u)$. Here, $f(u)g(u)$ represents the target population density.

Then define the balancing weight by

$$
w_z(u) = \frac{g(u)}{e(u)^{x} (1 - e(u))^{1-x}}, x = 0,1.
$$

then, the weighted ATE $\tau_g$ can be estimated by a Hájek-type estimator

$$
\hat{\tau}_g = \sum_{i=1}^N \left[X_i \hat{W}_1(u_i ) - (1 - X_i) \hat{W}_0 (u_i) \right] Y_i,
$$

where $\hat{W}_z(u)$ is the normalized balancing weights that sum up to 1 in each group $X=x, x=0,1$:

$$
\hat{W}_x(u) = \hat{w}_x(u) / N_{\hat{w}_x}, \ 
\hat{w}_x(u) = \hat{g}(u) \hat{e}(u)^{-x} (1 - \hat{e}(u))^{x-1}, \ 
N_{\hat{w}_x} =  \sum_{i=1}^N X_i^x (1-X_i)^{1-x} \hat{w}_x(u_i).
$$

In other words, the selection function can be regarded as the weight function of the CATE. That is, "for a given selected function $g(u)$, the weights $w_x(u)$ rescale the original study population to create a pseudo-population for which the treatment groups are balanced, on average, with respect to their baseline covariates to obtain a reliable treatment effect. However, this may result in a loss of precision for the estimator $\hat{\tau}_g$ due to the influence of extreme weights. "

Note that the inverse probability weighting (IPW) is a special case of the balancing weight estimator when letting $g(u) \equiv 1$. Besides, both IPW truncation and IPW trimming are special cases by defining

$$
g(u) = \alpha^{-1} e(u) \mathbb{I}(\{\alpha > e(u)\}) + \mathbb{I}(\{\alpha \leq e(u) \leq 1-\alpha\}) + (1 - \alpha)^{-1} (1-e(x)) \mathbb{I}(\{e(u) > 1-\alpha \})
$$and

$$
g(u) = \mathbb{I}(\{\alpha\leq e(u) \leq 1-\alpha \}),
$$

respectively.

The followings are some examples: (Now let $X$ indicates the covariate/confounder).

![](images/clipboard-2598390608.png)Notice that, except for the IPW estimator, when the propensity score $e(x)$ is close to 0 or 1, those selection functions $g(x)$ would close to 0, leading to that the balancing weight $w_{1}(x)$ or $w_0(x)$ close to 0. In other words, the selection functions smoothly and gradually limit the influence of participants whose propensity score is at both ends of the spectrum.

![](images/clipboard-1213168451.png)

Thanks to their summary, we can try to apply this to a simulation case.
